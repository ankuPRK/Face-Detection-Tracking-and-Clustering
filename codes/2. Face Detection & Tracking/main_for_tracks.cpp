/*
This code takes as input a video, and a *.txt file that contains the information of the shots of the video.
The information in the form of series of a pair of numbers.
Each pair represents the 'shot' of the video. First number corresponds to index of the first frame
of that shot, and second number is the no. of frames in that shot. Two no. of a pair are seperated by
',' and each pair is seperated from one another by '.'.
This .txt file can be generated by running 'xml_parser.py'
on the *.xml file generated by ShotDetect: https://github.com/johmathe/Shotdetect .
*/

/*
The code then keeps Detection and Tracking faces in the video, and for each frame, every face is saved in the format:
mmmmmmiisss . jpg
mmmmmm: The track index, each track consists of a certain no. of frames
ii:		The index of the face
sss:	The index of the frame in a particular track
*/

/*
Utilised these three cpp files to write code for basic detection+tracking :
1)	http://opencvexamples.blogspot.com/2015/10/face-detection-using-haar-cascade.html (Viola Jones algorithm for Face Detection)
2)	opencv\sources\samples\cpp\lkdemo.cpp (Lucas Kannade Algorithm for tracking)
3)	Dlib Sample file for Landmark detection
*/

/////dlib libraries//////
#include <dlib/opencv.h>
#include <dlib/image_processing/frontal_face_detector.h>
#include <dlib/image_processing/render_face_detections.h>
#include <dlib/image_processing.h>
#include <dlib/gui_widgets.h>

/////////OpenCV libraries//////////
#include "opencv2/video/tracking.hpp"
#include "opencv2/imgproc/imgproc.hpp"
#include "opencv2/videoio/videoio.hpp"
#include "opencv2/highgui/highgui.hpp"
#include "opencv2/objdetect/objdetect.hpp"

/////////other libs//////////
#include "mmr_lib.h"
#include <iostream>
#include <fstream>

//////using namespace/////////
using namespace cv;
using namespace std;
using namespace dlib;

//functions//
void draw_and_save(cv::Mat *pimage, cv::Mat *pBlurredFrame, int nFaces, int dHeight, int dWidth, int minutecount, int secondcount, std::vector<Point2f> *pcentroid, std::vector<Point2f> *pdim, std::vector < std::vector<Point2f> > *ppoints);

int main(int argc, char** argv)
{


	// dlib Detector variables
	// Load face detection and pose estimation models.
	frontal_face_detector detector = get_frontal_face_detector();
	shape_predictor pose_model;
	std::cout << "start reading shape_predictor_68_face_landmarks.dat \t";
	deserialize("shape_predictor_68_face_landmarks.dat") >> pose_model;
	std::cout << "Reading is done. \n";
	std::vector <dlib::rectangle> faces_Dlib;
	std::vector <dlib::full_object_detection> shapes;
	//vector for storing all the faces in the form of vector of rectangles on the image. Each rectangle 
	std::vector<Rect> faces;
	////
	
	// parameters to calculate the new(updated) rectangles(faces)
	std::vector<Point2f> centroid;		//vector of centroids of each rectangle(face)
	std::vector<Point2f> dim;			//vector of (width, height) of the rectangles of previous frame

	std::vector < std::vector<Point2f> > points;			//Contains a vector of "vector of points for each face", which are iteratively 
															//updated after each frame
	std::vector < std::vector<Point2f> > pointsPrev;		//vector of 'vector of points for each face' for the previous frame
	std::vector<Point2f>  tempPoints;
	const int MAX_COUNT = 500;								//max no. of points per rectangle

	//mat objects
	Mat image, RawFrame, BlurredFrame;			//type of Mat will be CV_8U 3 channels
	Mat BlurredGrayFrame, EqualizedGrayFrame, prevGrayFrame;   //type of Mat will be CV_8U 1 channel

	// parameters for controlling loop and saving image
	int i, j, k;
	bool needToDetect = true;
	int minutecount = 0;
	int secondcount = 0;
	int min_to_sec = 20;
	// we call a track a minute and frame in it seconds. This has nothing to do with the actual minute/second of the video.
	int nFaces = 0;
	bool bsuccess = false;
	// parameters for shot-data extraction
	std::vector <int> fbegins;
	std::vector <int> flengths;
	std::vector <int> tracklengths;
	fstream outfile;
	int vidIndex = 1;
	namedWindow("Detection+Tracking", WINDOW_AUTOSIZE);
	char *filename = "shot_info.txt";
	char *vidname = "sample.mp4";
	outfile.open(filename, ios::in);
	char charVal = 'a';
	int fbgn, fwdth;
	charVal = outfile.get();

//****************************************all variables declared, code starts from here*****************************// 

	fbegins.clear();
	flengths.clear();
	while (charVal != -1){
		fbgn = 0;
		while (charVal != ','){
			fbgn = fbgn * 10 + (charVal - 48);
			charVal = outfile.get();
		}
		charVal = outfile.get();
		fwdth = 0;
		while (charVal != '.'){
			fwdth = fwdth * 10 + (charVal - 48);
			charVal = outfile.get();
		}
		fbegins.push_back(fbgn);
		flengths.push_back(fwdth);
		charVal = outfile.get();
	}
	//shot data is extracted and stored in fbegins, flengths

	VideoCapture cap(vidname);
	if (!cap.isOpened())	// if not success, exit program
	{
		std::cout << "Cannot open the video cam" << endl;
		return -5;
	}

	//////height and width of the frame/////
	double dWidth = cap.get(CV_CAP_PROP_FRAME_WIDTH); //get the width of frames of the video
	double dHeight = cap.get(CV_CAP_PROP_FRAME_HEIGHT); //get the height of frames of the video
	std::cout << "Frame size : " << dWidth << " x " << dHeight << endl;
	std::cout << "Press Esc to exit.";

	int count = 0;
	for (i = 0; i < flengths.size(); i++){
		while (flengths[i]>min_to_sec){
			tracklengths.push_back(min_to_sec);
			flengths[i] -= min_to_sec;
		}
		if (flengths[i] > 0){
			tracklengths.push_back(flengths[i]);
		}
	}

	std::system("md data");		//create directory names 'data'
		//////The while(1) loop//////
	/*
	The loop does the following:
	->	It detects the rectangles for each face and in each rectangle
	it detects good features using GoodFeaturesToTrack function.
	->	Then it just tracks those points by calculating their Optical Flow
	by using KLT algorithm via calcOpticalFlowPyrLK function.
	-	To draw rectangles on tracked points, we use two assumptions:
	a. Centroid of the updated points of each face will be the centroid of the updated rectangle.
	b. Ratio of the dimentions of the new rectangle to the old rectangle will be the ratio of the
	variance of the points of new rectangle to the variance of the points of the old rectangle
	->	After 50 frames, it refreshes and starts from again.
	->	The loop terminates if the user presses the Esc key.
	*/
		while (1){
			cap >> RawFrame;			//read frame
			if (RawFrame.empty()){
				std::cout << "didn't get any frame. Stopping.\n";
				break;
			}
			//after frame is read and its availability is checked, proceed

			GaussianBlur(RawFrame, BlurredFrame, { 7, 7 }, 1.0, 1.0, BORDER_DEFAULT);	//smoothing
			BlurredFrame.copyTo(image);	//copy it to image
			cv::cvtColor(image, BlurredGrayFrame, COLOR_BGR2GRAY);	//convert into grayscale
			equalizeHist(BlurredGrayFrame, EqualizedGrayFrame);		//histogram equalization of image to remove lighting for face detector. 

			//NOTE: this image is only used in Tracker. Detector has its inbuilt image pre-processing, 
			//ordinary gaussian smoothened colored images are used for Detector and for saving.
			if (needToDetect) {
				//This part of the loop detects the faces and the landmark points using Dlib Detector:

				//other variables
				// Turn OpenCV's Mat into something dlib can deal with.  Note that this just
				// wraps the Mat object, it doesn't copy anything.  So cimg is only valid as
				// long as temp is valid.  Also don't do anything to temp that would cause it
				// to reallocate the memory which stores the image as that will make cimg
				// contain dangling pointers.  This basically means you shouldn't modify temp
				// while using cimg.
				cv_image<bgr_pixel> cimg(image);
				std::vector<point> *val;
				std::vector<Point2f>  tempPoints;
				int i = 0, j = 0;

				faces_Dlib.clear();
				shapes.clear();
				faces.clear();
				// Detect faces 
				faces_Dlib = detector(cimg);
				nFaces = faces_Dlib.size();
				faces.resize(nFaces);

				//preparing the vectors
				points.clear();
				centroid.resize(nFaces);
				dim.resize(nFaces);

				// Find the pose of each face.
				for (i = 0; i < faces_Dlib.size(); ++i){
					shapes.push_back(pose_model(cimg, faces_Dlib[i]));
				}
				for (i = 0; i < nFaces; i++) {

					//store landmark feature points of ith rectangle in tempPoints vector
					tempPoints.clear();
					val = shapes[i].get_parts();
					tempPoints.resize(val->size());
					for (j = 0; j < val->size(); j++){
						tempPoints[j].x = (int)((*val)[j].x());
						tempPoints[j].y = (int)((*val)[j].y());
					}

					Point sss, sss2;
					sss.x = 9999;
					sss.y = 9999;
					sss2.x = -9999;
					sss2.y = -9999;

					for (j = 0; j < val->size(); j++){
						if (sss.x > tempPoints[j].x){
							sss.x = tempPoints[j].x;
						}
						if (sss.y > tempPoints[j].y){
							sss.y = tempPoints[j].y;
						}
						if (sss2.x < tempPoints[j].x){
							sss2.x = tempPoints[j].x;
						}
						if (sss2.y < tempPoints[j].y){
							sss2.y = tempPoints[j].y;
						}
					}

					faces[i].x = sss.x;
					faces[i].y = sss.y;
					faces[i].width = sss2.x - sss.x;
					faces[i].height = sss2.y - sss.y;

					points.push_back(tempPoints);	//push tempPoints to points
				}
				///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

				for (i = 0; i < nFaces; i++){
					if (points[i].empty()) {
						//	printf("No point in this face during detection.\n");
						faces.erase(faces.begin() + i);
						points.erase(points.begin() + i);
						nFaces = faces.size();
						i--;
						centroid.resize(nFaces);
						dim.resize(nFaces);
					}
					else {
						//calculate the centroid
						centroid[i].x = faces[i].x + faces[i].width / 2.0;
						centroid[i].y = faces[i].y + faces[i].height / 2.0;

						//get the dimensions and centroids of all the rectangles
						dim[i].x = int(faces[i].width);
						dim[i].y = int(faces[i].height);

					}
				}


				needToDetect = false;
				//			printf("Detection of frame is completed successfully.\n");
			}
			else
			{
				////do the tracking of the landmark points////

				std::vector < std::vector<uchar> > status;		//status is 1 if the updated point is valid, else its status is 0.
				std::vector < std::vector<float> > err;
				std::vector < std::vector<Point2f> > tmp;
				std::vector<uchar>  tempStatus;
				std::vector<float>  tempErr;
				std::vector<Point2f> tempTmp;

				//CornerSubPix for more details//	
				TermCriteria termcrit(TermCriteria::COUNT | TermCriteria::EPS, 20, 0.03);
				Size subPixWinSize(10, 10), winSize(31, 31);
				int i, j, k;

				for (i = 0; i < nFaces; i++) {
					tempStatus.clear();
					tempErr.clear();
					//KLT tracking function
					calcOpticalFlowPyrLK(prevGrayFrame, EqualizedGrayFrame, pointsPrev[i], points[i], tempStatus, tempErr, winSize,
						3, termcrit, 0, 0.001);
					status.push_back(tempStatus);
					err.push_back(tempErr);
				}

				for (i = 0; i < nFaces; i++) {

					//update the points
					for (j = 0, k = 0; j < points[i].size(); j++)
					{
						if (!status[i][j])
							continue;
						points[i][k++] = points[i][j];
					}
					points[i].resize(k);
					//				printf("Tracking points of this frame updated.\n");
					if (points[i].empty()) {
						faces.erase(faces.begin() + i);
						points.erase(points.begin() + i);
						status.erase(status.begin() + i);
						err.erase(err.begin() + i);
						dim.erase(dim.begin() + i);
						nFaces = faces.size();
						i--;
						centroid.resize(nFaces);
						//					printf("One face erased during tracking.\n");
					}
					else {
						//calculate centroid
						centroid[i] = get_centroid(&(points[i]));
					}
				}
			}

			//draw rectangles around the faces, and landmark points inside the faces, and save the cropped rectangles
			draw_and_save(&image, &BlurredFrame, nFaces, dHeight, dWidth, minutecount, secondcount, &centroid, &dim,
				&points);

			//show the image in Window
			imshow("Detection+Tracking", image);

			//Wait for 10ms if a key is pressed. Note that even if you don't want any key-press input from the user still you 
			//have to keep the waitKey() function, because it is required by highGUI to handle the events of the window. 
			char c = (char)waitKey(1);
			if (c == 27) {
				break;		//exit if Esc is pressed
			}

			pointsPrev.clear();
			pointsPrev = points;		//update pointsPrev
			EqualizedGrayFrame.copyTo(prevGrayFrame);		//update the previous gray frame

			//		std::cout << secondcount << "\t" << minutecount << "\n";
			//update the counters
			if (secondcount == tracklengths[count] - 1) {
				//start afresh
				pointsPrev.clear();
				needToDetect = true;
				count += 1;
				minutecount++;
				secondcount = 0;
			}
			else{
				secondcount+=1;
			}
			if (count > tracklengths.size()){
				break;
			}
		}

	return 0;
}



//Tried to make two functions for detection and tracking, but they make code very slow, so these functions aren't being used
/*
The pointers are used to modify all the variables passed into the function. Finally, variables of importance are:
pimage: pointer to the input image frame
faces: Vector of Rect, contains all the bounding boxes for the faces
nFaces: integer showing no. of faces
points: vector of landmark-points-vector for each faces[i]
centroid: centroid of each point[i]
dim: vector of width, height of each faces[i]
*/ /*
void run_the_detector(std::vector <dlib::rectangle> *pfaces_Dlib, cv::Mat *pimage, std::vector <dlib::full_object_detection> *pshapes, 
	std::vector<Rect> *pfaces, std::vector<Point2f> *pcentroid, std::vector<Point2f> *pdim, frontal_face_detector *pdetector, int *pnFaces, shape_predictor *ppose_model, std::vector < std::vector<Point2f> > *ppoints ){

	//pointers:
	std::vector <dlib::rectangle> faces_Dlib = *pfaces_Dlib;
	Mat image = *pimage;
	std::vector <dlib::full_object_detection> shapes = *pshapes;
	std::vector<Rect> faces = *pfaces;
	std::vector<Point2f> centroid = *pcentroid;		
	std::vector<Point2f> dim = *pdim;			
	frontal_face_detector detector = *pdetector;
	int nFaces = *pnFaces;
	shape_predictor pose_model = *ppose_model;
	std::vector < std::vector<Point2f> > points = *ppoints; 

	//other variables
	// Turn OpenCV's Mat into something dlib can deal with.  Note that this just
	// wraps the Mat object, it doesn't copy anything.  So cimg is only valid as
	// long as temp is valid.  Also don't do anything to temp that would cause it
	// to reallocate the memory which stores the image as that will make cimg
	// contain dangling pointers.  This basically means you shouldn't modify temp
	// while using cimg.
	cv_image<bgr_pixel> cimg(image);
	std::vector<point> *val;
	std::vector<Point2f>  tempPoints;
	int i = 0, j = 0;

	faces_Dlib.clear();
	shapes.clear();
	faces.clear();
	// Detect faces 
	faces_Dlib = detector(cimg);
	nFaces = faces_Dlib.size();
	faces.resize(nFaces);

	//preparing the vectors
	points.clear();
	centroid.resize(nFaces);
	dim.resize(nFaces);

	// Find the pose of each face.
	for (i = 0; i < faces_Dlib.size(); ++i){
		shapes.push_back(pose_model(cimg, faces_Dlib[i]));
	}
	for (i = 0; i < nFaces; i++) {

		//store landmark feature points of ith rectangle in tempPoints vector
		tempPoints.clear();
		val = shapes[i].get_parts();
		tempPoints.resize(val->size());
		for (j = 0; j < val->size(); j++){
			tempPoints[j].x = (int)((*val)[j].x());
			tempPoints[j].y = (int)((*val)[j].y());
		}

		Point sss, sss2;
		sss.x = 9999;
		sss.y = 9999;
		sss2.x = -9999;
		sss2.y = -9999;

		for (j = 0; j < val->size(); j++){
			if (sss.x > tempPoints[j].x){
				sss.x = tempPoints[j].x;
			}
			if (sss.y > tempPoints[j].y){
				sss.y = tempPoints[j].y;
			}
			if (sss2.x < tempPoints[j].x){
				sss2.x = tempPoints[j].x;
			}
			if (sss2.y < tempPoints[j].y){
				sss2.y = tempPoints[j].y;
			}
		}

		faces[i].x = sss.x;
		faces[i].y = sss.y;
		faces[i].width = sss2.x - sss.x;
		faces[i].height = sss2.y - sss.y;

		points.push_back(tempPoints);	//push tempPoints to points
	}
	///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

	for (i = 0; i < nFaces; i++){
		if (points[i].empty()) {
			//	printf("No point in this face during detection.\n");
			faces.erase(faces.begin() + i);
			points.erase(points.begin() + i);
			nFaces = faces.size();
			i--;
			centroid.resize(nFaces);
			dim.resize(nFaces);
		}
		else {
			//calculate the centroid
			centroid[i].x = faces[i].x + faces[i].width / 2.0;
			centroid[i].y = faces[i].y + faces[i].height / 2.0;

			//get the dimensions and centroids of all the rectangles
			dim[i].x = int(faces[i].width);
			dim[i].y = int(faces[i].height);

		}
	}

}
*/
/*
The pointers are used to modify all the variables passed into the function. Finally, variables of importance are:
pimage: pointer to the input image frame
pprevGrayFrame: pointer to the previous input gray frame
pEqualizedGrayFrame: pointer to the current input gray frame
faces: Vector of Rect, contains all the bounding boxes for the faces
nFaces: integer showing no. of faces
points: vector of landmark-points-vector for each faces[i]
pointsPrev: points of the prevGrayFrame
centroid: centroid of each point[i]
dim: vector of width, height of each faces[i]
*/ /*
void run_the_tracker(cv::Mat *pimage, cv::Mat *pprevGrayFrame, cv::Mat *pEqualizedGrayFrame, 
	std::vector<Rect> *pfaces, std::vector<Point2f> *pcentroid, std::vector<Point2f> *pdim, int nFaces, 
	std::vector < std::vector<Point2f> > *ppoints, std::vector < std::vector<Point2f> > *ppointsPrev){

	//Parameters utilised in calculating the optical flow using the KLT algorithm//
	std::vector < std::vector<uchar> > status;		//status is 1 if the updated point is valid, else its status is 0.
	std::vector < std::vector<float> > err;
	std::vector < std::vector<Point2f> > tmp;
	std::vector<uchar>  tempStatus;
	std::vector<float>  tempErr;
	std::vector<Point2f> tempTmp;
	Mat image = *pimage;
	Mat prevGrayFrame = *pprevGrayFrame;
	Mat EqualizedGrayFrame = *pEqualizedGrayFrame;
	std::vector<Rect> faces = *pfaces;
	std::vector<Point2f> centroid = *pcentroid;
	std::vector<Point2f> dim = *pdim;
	std::vector < std::vector<Point2f> > points = *ppoints;
	std::vector < std::vector<Point2f> > pointsPrev = *ppointsPrev;

	//CornerSubPix for more details//	
	TermCriteria termcrit(TermCriteria::COUNT | TermCriteria::EPS, 20, 0.03);
	Size subPixWinSize(10, 10), winSize(31, 31);
	int i, j, k;

	for (i = 0; i < nFaces; i++) {
		tempStatus.clear();
		tempErr.clear();
		//KLT tracking function
		calcOpticalFlowPyrLK(prevGrayFrame, EqualizedGrayFrame, pointsPrev[i], points[i], tempStatus, tempErr, winSize,
			3, termcrit, 0, 0.001);
		status.push_back(tempStatus);
		err.push_back(tempErr);
	}

	for (i = 0; i < nFaces; i++) {

		//update the points
		for (j = 0, k = 0; j < points[i].size(); j++)
		{
			if (!status[i][j])
				continue;
			points[i][k++] = points[i][j];
		}
		points[i].resize(k);
		//				printf("Tracking points of this frame updated.\n");
		if (points[i].empty()) {
			faces.erase(faces.begin() + i);
			points.erase(points.begin() + i);
			status.erase(status.begin() + i);
			err.erase(err.begin() + i);
			dim.erase(dim.begin() + i);
			nFaces = faces.size();
			i--;
			centroid.resize(nFaces);
			//					printf("One face erased during tracking.\n");
		}
		else {
			//calculate centroid
			centroid[i] = get_centroid(&(points[i]));
		}
	}
}
*/

void draw_and_save(cv::Mat *pimage, cv::Mat *pBlurredFrame, int nFaces, int dHeight, int dWidth, int minutecount, int secondcount, std::vector<Point2f> *pcentroid, std::vector<Point2f> *pdim, std::vector < std::vector<Point2f> > *ppoints){

	//pointers
	cv::Mat image = *pimage;
	cv::Mat BlurredFrame = *pBlurredFrame;
	std::vector<Point2f> centroid = *pcentroid;
	std::vector<Point2f> dim = *pdim;
	std::vector < std::vector<Point2f> > points = *ppoints;

	int i, j;
	Point2f topleft;
	Point2f Botright;
	char *str;
	Rect Cropper;		//will be used to crop images
	Mat cropped_image;



	for (i = 0; i < nFaces; i++) {

		//draw the points on the image
		for (j = 0; j < points[i].size(); j++) {
			cv::circle(image, points[i][j], 1, Scalar(255, 255, 255), -1, 8);
		}
		//draw the centriod in red color
		cv::circle(image, centroid[i], 2, Scalar(0, 0, 255), -1, 8);


		//binding the rectangle into the image
		topleft.x = (centroid[i].x - dim[i].x / 2 > 0 ? centroid[i].x - dim[i].x / 2 : 0);
		topleft.y = (centroid[i].y - dim[i].y / 2 > 0 ? centroid[i].y - dim[i].y / 2 : 0);
		Botright.x = (centroid[i].x + dim[i].x / 2 < dWidth ? centroid[i].x + dim[i].x / 2 : dWidth);
		Botright.y = (centroid[i].y + dim[i].y / 2 < dHeight ? centroid[i].y + dim[i].y / 2 : dHeight);

		//			std::cout << "topleft: " << topleft << "botright: " << Botright << "\n";
		Cropper = Rect{ (int)topleft.x, (int)topleft.y, (int)Botright.x - (int)topleft.x, (int)Botright.y - (int)topleft.y };

		cv::rectangle(image, Cropper, Scalar(255, 0, 0), 2, 8, 0);
		str = get_string_id(minutecount, i, secondcount, "data//");
		BlurredFrame(Cropper).copyTo(cropped_image);
		cv::imwrite(str, cropped_image);
	}

}